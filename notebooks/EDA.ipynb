{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "EDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "o7K7DvdZ2R4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#Data set is too big and gets memory allocation error. Function is to sample chunks from dataset\n",
        "\n",
        "path = os.path()\n",
        "train_path = 'simplified-nq-train.jsonl'\n",
        "test_path = 'simplified-nq-test.jsonl'\n",
        "sample_submission_path = 'sample_submission.csv'\n",
        "sample_size = 300 \n",
        "\n",
        "def read_chunk(path, size):\n",
        "    df = []\n",
        "    with open(path, 'rt') as jsonfile:\n",
        "        for i in range(size):\n",
        "                df.append(json.loads(jsonfile.readline()))\n",
        "        df = pd.DataFrame(df)\n",
        "        print('Sampled dataset shape: \\n{} rows and {} columns'.format(df.shape[0], df.shape[1]))\n",
        "    return df\n",
        "\n",
        "df_train = read_chunk(os.path.join(path,train_path), sample_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRiI9Hoh2R4r",
        "colab_type": "text"
      },
      "source": [
        "** EDA /Exploratory data analysis **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "jc99B0LT2R4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def missing_values(df):\n",
        "    df = pd.DataFrame(df.isnull().sum()).reset_index()\n",
        "    df.columns = ['name', 'missing']\n",
        "    return df\n",
        "\n",
        "missing_values(df_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "1PNaOjxk2R4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "wc = WordCloud(background_color='black',\n",
        "                    stopwords = STOPWORDS,\n",
        "                    max_words = 900,\n",
        "                    max_font_size = 150, \n",
        "                    random_state = 92,\n",
        "                    width=900, \n",
        "                    height=500)\n",
        "wc.generate(str(df_train[\"question_text\"]))\n",
        "plt.figure(figsize=(20.0,10.0))\n",
        "plt.axis('off')\n",
        "plt.imshow(wc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "u72Xr6Wt2R4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.util import ngrams\n",
        "import re\n",
        "\n",
        "# Fixing random state for reproducibility \n",
        "\n",
        "imported_stopwords =pd.Series(np.loadtxt(\"../input/stopwords/long stopword list.txt\",dtype=str,delimiter='\\n'))\n",
        "stop_words = set(stopwords.words('english')) \n",
        "stop_words = stop_words.union(STOPWORDS,imported_stopwords)\n",
        "\n",
        "#ngrams\n",
        "def extract_ngrams(df, n):\n",
        "    df_ngrams=[]\n",
        "    for text in df:     \n",
        "        text = re.sub(r'[^\\w\\s]','',text.lower())\n",
        "        tokens = [w for w in nltk.word_tokenize(text) if not w in stop_words] \n",
        "        n_grams = ngrams(tokens, n)\n",
        "        df_ngrams.append([ ' '.join(grams) for grams in ngrams(tokens, n)])\n",
        "    return df_ngrams\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}